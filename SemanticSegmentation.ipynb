{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Assignment_4_StudentID.ipynb","provenance":[],"collapsed_sections":[]},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.7"}},"cells":[{"cell_type":"code","metadata":{"id":"EHD1DOQVACzP","executionInfo":{"status":"ok","timestamp":1604460005530,"user_tz":-330,"elapsed":1416,"user":{"displayName":"Poulomi Ganguly","photoUrl":"https://lh6.googleusercontent.com/-sZ8VSPeaiMc/AAAAAAAAAAI/AAAAAAAAAvY/cuv7KvYLHts/s64/photo.jpg","userId":"02583667685880525621"}},"outputId":"059c40df-2ee2-43fe-c9bb-3d31559fb768","colab":{"base_uri":"https://localhost:8080/"}},"source":["# Student name: Poulomi Ganguly\n","# Student ID: 1598887\n","\n","import os\n","import time\n","\n","import torch.nn.functional as F\n","import torch\n","from torch import nn\n","from torchvision import models\n","import numpy as np\n","\n","from google.colab import drive\n","drive.mount('/content/drive')\n","\n","#Edit root_dir here to point where the TextureImagesDataset folder \n","root_dir = '/content/drive/My Drive/CMPUT328/Semantic Segmentation'\n","\n","class TextureImages(object):\n","    def __init__(self, subset='train', batch_size=64, shuffle=True):\n","        if subset == 'train':\n","            images = np.load(os.path.join(root_dir, 'TextureImagesDataset',\n","                                          'train_images.npy'))\n","            masks = np.load(os.path.join(root_dir, 'TextureImagesDataset',\n","                                         'train_masks.npy'))\n","        elif subset == 'test':\n","            images = np.load(os.path.join(root_dir, 'TextureImagesDataset',\n","                                          'test_images.npy'))\n","            masks = np.load(os.path.join(root_dir, 'TextureImagesDataset',\n","                                         'test_masks.npy'))\n","        else:\n","            raise NotImplementedError\n","        self._images = images\n","        self.images = self._images\n","        self._masks = masks\n","        self.masks = self._masks\n","        self.batch_size = batch_size\n","        self.num_samples = len(self.images)\n","        self.shuffle = shuffle\n","        if self.shuffle:\n","            self.shuffle_samples()\n","        self.next_batch_pointer = 0\n","\n","    def shuffle_samples(self):\n","        image_indices = np.random.permutation(np.arange(self.num_samples))\n","        self.images = self._images[image_indices]\n","        self.masks = self._masks[image_indices]\n","\n","    def get_next_batch(self):\n","        num_samples_left = self.num_samples - self.next_batch_pointer\n","        if num_samples_left >= self.batch_size:\n","            x_batch = self.images[self.next_batch_pointer:self.next_batch_pointer + self.batch_size]\n","            y_batch = self.masks[self.next_batch_pointer:self.next_batch_pointer + self.batch_size]\n","            self.next_batch_pointer += self.batch_size\n","        else:\n","            x_partial_batch_1 = self.images[self.next_batch_pointer:self.num_samples]\n","            y_partial_batch_1 = self.masks[self.next_batch_pointer:self.num_samples]\n","            if self.shuffle:\n","                self.shuffle_samples()\n","            x_partial_batch_2 = self.images[0:self.batch_size - num_samples_left]\n","            y_partial_batch_2 = self.masks[0:self.batch_size - num_samples_left]\n","            x_batch = np.vstack((x_partial_batch_1, x_partial_batch_2))\n","            y_batch = np.vstack((y_partial_batch_1, y_partial_batch_2))\n","            self.next_batch_pointer = self.batch_size - num_samples_left\n","        return x_batch, y_batch\n","\n","class CrossEntropyLoss2d(nn.Module):\n","    def __init__(self, weight=None, size_average=True, ignore_index=255):\n","        super(CrossEntropyLoss2d, self).__init__()\n","        self.nll_loss = nn.NLLLoss(weight, size_average, ignore_index)\n","\n","    def forward(self, inputs, targets):\n","        return self.nll_loss(F.log_softmax(inputs, dim=1), targets)\n"],"execution_count":7,"outputs":[{"output_type":"stream","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"RyHyAc29oQ9D","executionInfo":{"status":"ok","timestamp":1604443424338,"user_tz":-330,"elapsed":1242,"user":{"displayName":"Poulomi Ganguly","photoUrl":"https://lh6.googleusercontent.com/-sZ8VSPeaiMc/AAAAAAAAAAI/AAAAAAAAAvY/cuv7KvYLHts/s64/photo.jpg","userId":"02583667685880525621"}},"outputId":"d12dc779-01fa-4d1f-d462-2725dbe8cfbc","colab":{"base_uri":"https://localhost:8080/"}},"source":["from google.colab import drive\n","drive.mount('/content/drive')"],"execution_count":2,"outputs":[{"output_type":"stream","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"RSyJsklzzojQ","executionInfo":{"status":"ok","timestamp":1604460012183,"user_tz":-330,"elapsed":1236,"user":{"displayName":"Poulomi Ganguly","photoUrl":"https://lh6.googleusercontent.com/-sZ8VSPeaiMc/AAAAAAAAAAI/AAAAAAAAAvY/cuv7KvYLHts/s64/photo.jpg","userId":"02583667685880525621"}}},"source":["def SemSeg(input_size, num_classes=5):\n","    # TODO: Implement Semantic Segmentation network here\n","    # Returned logits must be a tensor of size:\n","    # (<batch_size>, image_height, image_width, num_classes + 1)\n","    # 1st dimension is batch dimension\n","    # image_height and image_width are the height and width of input_tensor\n","    # last dimension is the softmax dimension. There are 4 texture classes plus 1 background class\n","    # therefore last dimension will be 5\n","    # ----------------------------------------------------------------------------------------------\n","    \n","    # Declaring a class for convolution between any two specified channels\n","    # Accomodates down convolutions required for this architecture \n","\n","    # Initialization of a ConvLayer has the following input parameters:\n","    # * in_ch: Channels in the layer that is going through convolution\n","    # * out_ch: Desired channels after convolution\n","\n","    # Explanation of working: Upon initializing ConvLayer, nn.Sequential\n","    # carries out 2d convolution based on specified input and output \n","    # channels with a filter of size 3x3, padding of size 1 and a default\n","    # stride of size 1\n","    # The use of BatchNorm2d ensured a more stable training phase with\n","    # consistent drop in loss and less variations in time taken to train\n","    # each batch\n","    # Next, ReLU is taken in place to finish the convolution layer. \n","\n","    # The forward function of the convolution layer returns the result of the\n","    # prior operations based on given input.\n","\n","    # Note: Padding of 1 helps with double convolutions on an image of size\n","    # 196 x 196 dimension. The stride size of 1 limits the padding amount to\n","    # 0 and 1. After testing with both, padding = 1 ensures zero data loss at\n","    # maxpool layer.\n","    class ConvLayer(nn.Module):\n","      def __init__(self, in_ch, out_ch):\n","          super(ConvLayer, self).__init__()\n","          self.conv = nn.Sequential(\n","              nn.Conv2d(in_ch, out_ch, kernel_size=3, padding = 1),\n","              nn.BatchNorm2d(out_ch),\n","              nn.ReLU(inplace=True),\n","          )\n","      def forward(self, input):\n","          return self.conv(input)\n","\n","\n","    # This class is loosely based on the UNet architechture for semantic\n","    # segmentation, however it does not involve the cropping of previous down\n","    # convolution layers as well as has less down and upsampling layers in general\n","    # to accomodate the smaller dimensions compared to that of the original paper.\n","    # i.e. 3 x 196 x 196 rather than 1 x 572 x 572\n","\n","    # Explanation of Network + changes made:\n","\n","    # https://miro.medium.com/max/2824/1*f7YOaE4TWubwaFF7Z1fzNw.png\n","    # According to the above figure, UNet relies on a series of down convolutions\n","    # followed by up convolutions to make a fully convolutional network model \n","    # which achieves semantic segmentation due to an hourglass shaped stack of layers.\n","\n","    # This implies that the input image is downsampled several times through a sequence \n","    # of double down convolutions and maxpool functions and then upsampled through the\n","    # opposite of the same process (transposed convolution) in order to restore its original\n","    # dimension\n","\n","    # Initialization: \n","    # The initialization for this network defines all the required down convolution, maxpool\n","    # Transpose convolution and up convolution layers. These are called in the forward function\n","    # This initialization takes in the following parameters.\n","    # * in_ch: Channels in the layer that is going through convolution\n","    # * out_ch: Desired no. of channels after convolution\n","\n","\n","    class UNet(nn.Module):\n","      def __init__(self, in_ch, out_ch):\n","        super(UNet, self).__init__()\n","\n","        self.Conv_Down_1 = ConvLayer(in_ch, 32)\n","        self.Conv_Down_2 = ConvLayer(32, 32)\n","        self.max_pool = nn.MaxPool2d(kernel_size = 2, stride = 2)\n","\n","        self.Conv_Down_3 = ConvLayer(32, 64)\n","        self.Conv_Down_4 = ConvLayer(64, 64)\n","\n","        self.Conv_Down_5 = ConvLayer(64, 128)\n","\n","        self.TransConv_Up_1 = nn.ConvTranspose2d(128, 64, 2, stride=2)\n","        self.Conv_Up_1 = ConvLayer(128, 64)\n","\n","        self.TransConv_Up_2 = nn.ConvTranspose2d(64, 32, 2, stride=2)\n","        self.Conv_Up_2 = ConvLayer(64, 32)\n","\n","        self.Last_Conv_Up = nn.Conv2d(32, out_ch, 1)\n","        \n","      def forward(self, x):\n","        \n","        # ------ Downsampling ----------\n","        # Starting off with a batch of 3 x 196 x 196 images, the first down\n","        # convolution results in c1 becoming a layer with 32 channels and\n","        # have height and width remain the same\n","        # Thus, size of c1 is 32 x 196 x 196 (D x W x H)\n","\n","        # Next, another convolution takes place which retains this dimension\n","        # Thus, size of c2 is also 32 x 196 x 196 (D x W x H). \n","        # This double convolution resulted in further loss in dimension in the original\n","        # paper, however the parameters of the convolution has allowed for\n","        # the size to stay same. \n","\n","        # This layer is then maxpooled and the input dimensions are effectively halved.\n","        # This results in lesser parameters for computation for further downsampling.\n","        # However, note that the number of channels from before will stay the same.\n","        # Size of p1: 32 x 98 x 98\n","\n","        c1 = self.Conv_Down_1(x)\n","        c2 = self.Conv_Down_2(c1)\n","        p1 = self.max_pool(c2)\n","\n","        # Another set of double down convolution is performed. However, output \n","        # channels are doubled. 32 -> 64 channels.\n","        # The height and width of the result of these down convolution stays same\n","        # as before\n","        # Thus, the new sizes as result of down convolutions is:\n","        # Size of c3 = size of c4 = 64 x 98 x 98\n","        # Maxpooling these layers further reduces height and width down to \n","        # 49 each. \n","        # Size of p2 = 64 x 49 x 49\n","        \n","        c3 = self.Conv_Down_3(p1)\n","        c4 = self.Conv_Down_4(c3)\n","        p2 = self.max_pool(c4)\n","\n","        # At this point in the paper, an additional two sets of double convolutions\n","        # + maxpooling is performed to obtain a total of 1024 channels. However\n","        # this contributes significantly to training time and thus I have stopped at\n","        # 128 channels before moving on to upsampling. I have not gone through\n","        # with a double convolution of the last layer (unlike original paper) to cut \n","        # down on training time however, this is a relatively miniscule difference. \n","\n","        # This results in c5 having a size of 128 x 49 x 49\n","\n","        c5 = self.Conv_Down_5(p2)\n","\n","        # ------ Upsampling ----------\n","\n","        # Upsampling involves upwards transposed convolutions as well as upwards double\n","        # convolutions. Unlike the original paper, I have used single upwards convolutions\n","        # rather than double and have still achieved high accuracy. I assume that larger\n","        # resolutions would be helped by these layers of double convolutions in order\n","        # to achieve segmentation closer to ground truth\n","\n","        # Starting off by a transposed convolution layer on c5 which was of size\n","        # 128 x 49 x 49, we get up1 which has the half the number of channels and\n","        # double the height and width. This required some experimentation to get an \n","        # exact double. Alternatively, UpSampling2D can also perform the same operation\n","        \n","        # Size of up1 64 x 98 x 98\n","\n","        # https://towardsdatascience.com/understanding-semantic-segmentation-with-unet-6be4f42d4b47\n","        # \"To get better precise locations, at every step of the decoder we use skip connections by \n","        # concatenating the output of the transposed convolution layers with the feature maps from \n","        # the Encoder at the same level:\"\n","\n","        # Size of c6 64 x 98 x 98 (dimensions are preserved)\n","\n","        up1 = self.TransConv_Up_1(c5)\n","        c6 = self.Conv_Up_1(torch.cat([up1,c4],dim=1))\n","\n","        # Another set of transposed convolution + concatanation with feature map\n","        # from downsampled layer at same level is carried out. This results in \n","        # number of channels halving again and the input height and width being doubled to 196\n","        # This step has thus restored original dimension.\n","\n","        # Size of up2 = size of c7 = 32 x 196 x 196\n","        up2 = self.TransConv_Up_2(c6)\n","        c7 = self.Conv_Up_2(torch.cat([up2,c2],dim=1))\n","\n","        # Unlike the papers last double convolution, I have carried out one last\n","        # up convolution to filter the number of channels down to required number\n","        # of output channels. Alternatively, this is also the number classes we\n","        # need the returned logit to have (5)\n","\n","        # Size of c8 is num_classes x 196 x 196\n","        c8 = self.Last_Conv_Up(c7)        \n","\n","        # Return softmax of last layer to normalize output to something resembling \n","        # a probability distribution (Thus lies between 0 and 1). Alternatively, a\n","        # sigmoid function may also be used.\n","        return nn.Softmax()(c8)\n","    \n","    # Initializing the UNet network with 3 input channels and number of classes\n","    # expected at the end after softmax layer. \n","    model = UNet(3, num_classes)\n","    return model\n","\n","\n"],"execution_count":8,"outputs":[]},{"cell_type":"code","metadata":{"id":"H0wpkK7d9zWJ","executionInfo":{"status":"ok","timestamp":1604462280408,"user_tz":-330,"elapsed":2120638,"user":{"displayName":"Poulomi Ganguly","photoUrl":"https://lh6.googleusercontent.com/-sZ8VSPeaiMc/AAAAAAAAAAI/AAAAAAAAAvY/cuv7KvYLHts/s64/photo.jpg","userId":"02583667685880525621"}},"outputId":"b8ee23ea-4ca2-417f-993e-69cbdba4c78b","colab":{"base_uri":"https://localhost:8080/"}},"source":["def run():\n","    # You can tune the hyperparameters here.\n","    n_epochs = 2 # 2 epochs are enough to bring accuracy to 95%\n","    batch_size = 16 # Batch size determines number of images being passed in to network\n","    learning_rate = 0.001 # L2 regularization\n","    weight_decay = 0.001 \n","    use_cuda = 1\n","\n","    load_weights = 0\n","    wts_fname = 'model.pt'\n","\n","    input_size = (196, 196)\n","    n_batches = int(2000 / batch_size)\n","    wts_path = os.path.join(root_dir, wts_fname)\n","\n","    if use_cuda and torch.cuda.is_available():\n","        device = torch.device(\"cuda\")\n","        print('Training on GPU: {}'.format(torch.cuda.get_device_name(0)))\n","    else:\n","        device = torch.device(\"cpu\")\n","        print('Training on CPU')\n","\n","    train_set = TextureImages('train', batch_size=batch_size)\n","    test_set = TextureImages('test', shuffle=False)\n","\n","    model = SemSeg(input_size).to(device)\n","\n","    def evaluation(images, true_labels):\n","        eval_batch_size = 100\n","        predicted_labels = []\n","        model.eval()\n","        with torch.no_grad():\n","            for start_index in range(0, len(images), eval_batch_size):\n","                end_index = start_index + eval_batch_size\n","                batch_x = images[start_index: end_index]\n","                # batch_x = np.reshape(batch_x, (batch_x.shape[0], 3, 196, 196))\n","                batch_x = torch.FloatTensor(batch_x).permute((0, 3, 1, 2)).to(device)\n","                print(batch_x.size)\n","                batch_predicted_logits = model(batch_x)\n","                batch_predicted_labels = torch.argmax(batch_predicted_logits, axis=1)\n","                batch_predicted_labels = batch_predicted_labels.cpu().numpy()\n","                predicted_labels += list(batch_predicted_labels)\n","        predicted_labels = np.vstack(predicted_labels).flatten()\n","        true_labels = true_labels.flatten()\n","        accuracy = float((predicted_labels == true_labels).astype(np.int32).sum()) / true_labels.size\n","        return predicted_labels, accuracy\n","\n","    if load_weights:      \n","        print('Loading weights from: {}'.format(wts_path))\n","        chkpt = torch.load(wts_path, map_location=device)  # load checkpoint\n","        model.load_state_dict(chkpt['model'])\n","    else:\n","        criterion = CrossEntropyLoss2d().to(device)\n","        optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate, weight_decay=weight_decay)\n","\n","        print(\"Training...\")\n","        mean_loss = 0\n","        steps = 0\n","        losses = []\n","        max_accuracy = 0\n","        max_accuracy_id = 0\n","        for epch_id in range(n_epochs):\n","            model.train()\n","            for batch_id in range(n_batches):\n","                start_t = time.time()\n","\n","                batch_x, batch_y = train_set.get_next_batch()\n","\n","                batch_x = torch.FloatTensor(batch_x).permute((0, 3, 1, 2)).to(device)\n","                batch_y = torch.LongTensor(batch_y).squeeze().to(device)\n","\n","                optimizer.zero_grad()\n","\n","                outputs = model(batch_x)\n","                loss = criterion(outputs, batch_y)\n","                loss.backward()\n","                optimizer.step()\n","\n","                end_t = time.time()\n","\n","                _loss = loss.item()\n","                steps += 1\n","                mean_loss += (_loss - mean_loss) / steps\n","                losses.append(_loss)\n","\n","                time_taken = end_t - start_t\n","\n","                print('batch: {} / {} loss: {} mean_loss: {} time_taken: {}'.format(\n","                    batch_id, n_batches, _loss, mean_loss, time_taken))\n","\n","            _, test_accuracy = evaluation(test_set._images, test_set._masks)\n","            if test_accuracy > max_accuracy:\n","                max_accuracy = test_accuracy\n","                max_accuracy_id = epch_id\n","                chkpt = {\n","                    'model': model.state_dict(),\n","                }\n","                torch.save(chkpt, '{}.{}'.format(wts_path, max_accuracy_id))\n","            print(\"epch {} / {}: Test Pixel Accuracy = {:.3f} max_accuracy = {:.3f} in epoch {}\".format(\n","                epch_id + 1, n_epochs, test_accuracy, max_accuracy, max_accuracy_id + 1))\n","        print(\"Done training. Weights saved to: {}\".format(wts_fname))\n","        chkpt = {\n","            'model': model.state_dict(),\n","        }\n","        torch.save(chkpt, wts_path)\n","\n","    print('Evaluating on test set')\n","    _, test_accuracy = evaluation(test_set._images, test_set._masks)\n","    print(\"Test Pixel Accuracy = {:.3f}\".format(test_accuracy))\n","    return test_accuracy\n","\n","\n","if __name__ == '__main__':\n","    run()\n"],"execution_count":10,"outputs":[{"output_type":"stream","text":["Training on CPU\n"],"name":"stdout"},{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/torch/nn/_reduction.py:44: UserWarning: size_average and reduce args will be deprecated, please use reduction='mean' instead.\n","  warnings.warn(warning.format(ret))\n"],"name":"stderr"},{"output_type":"stream","text":["Training...\n"],"name":"stdout"},{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:186: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n"],"name":"stderr"},{"output_type":"stream","text":["batch: 0 / 125 loss: 1.602805495262146 mean_loss: 1.602805495262146 time_taken: 8.53781008720398\n","batch: 1 / 125 loss: 1.5546015501022339 mean_loss: 1.57870352268219 time_taken: 7.88772988319397\n","batch: 2 / 125 loss: 1.5108996629714966 mean_loss: 1.5561022361119587 time_taken: 7.96498441696167\n","batch: 3 / 125 loss: 1.4861730337142944 mean_loss: 1.5386199355125427 time_taken: 7.902710914611816\n","batch: 4 / 125 loss: 1.4675447940826416 mean_loss: 1.5244049072265624 time_taken: 7.696413516998291\n","batch: 5 / 125 loss: 1.4378799200057983 mean_loss: 1.5099840760231018 time_taken: 7.649884939193726\n","batch: 6 / 125 loss: 1.4389235973358154 mean_loss: 1.4998325790677751 time_taken: 7.911668539047241\n","batch: 7 / 125 loss: 1.4165600538253784 mean_loss: 1.4894235134124756 time_taken: 7.729214906692505\n","batch: 8 / 125 loss: 1.394759178161621 mean_loss: 1.4789052539401584 time_taken: 7.639113903045654\n","batch: 9 / 125 loss: 1.3877413272857666 mean_loss: 1.4697888612747192 time_taken: 7.713183403015137\n","batch: 10 / 125 loss: 1.3833248615264893 mean_loss: 1.4619284976612437 time_taken: 7.574066162109375\n","batch: 11 / 125 loss: 1.3770067691802979 mean_loss: 1.4548516869544983 time_taken: 7.629556179046631\n","batch: 12 / 125 loss: 1.3670090436935425 mean_loss: 1.4480945605498095 time_taken: 7.540968418121338\n","batch: 13 / 125 loss: 1.3657801151275635 mean_loss: 1.4422149573053633 time_taken: 7.883867025375366\n","batch: 14 / 125 loss: 1.3514128923416138 mean_loss: 1.43616148630778 time_taken: 7.638715982437134\n","batch: 15 / 125 loss: 1.3443541526794434 mean_loss: 1.430423527956009 time_taken: 8.048950910568237\n","batch: 16 / 125 loss: 1.3347941637039185 mean_loss: 1.4247982712352978 time_taken: 7.627997875213623\n","batch: 17 / 125 loss: 1.3144426345825195 mean_loss: 1.4186674025323656 time_taken: 7.915870189666748\n","batch: 18 / 125 loss: 1.314874291419983 mean_loss: 1.4132046072106612 time_taken: 7.73496675491333\n","batch: 19 / 125 loss: 1.3157209157943726 mean_loss: 1.4083304226398468 time_taken: 7.8430140018463135\n","batch: 20 / 125 loss: 1.3018287420272827 mean_loss: 1.4032589140392484 time_taken: 7.622055292129517\n","batch: 21 / 125 loss: 1.296335220336914 mean_loss: 1.3983987461436878 time_taken: 7.9830615520477295\n","batch: 22 / 125 loss: 1.300708293914795 mean_loss: 1.3941513351772141 time_taken: 7.7698094844818115\n","batch: 23 / 125 loss: 1.2913312911987305 mean_loss: 1.3898671666781106 time_taken: 7.709134101867676\n","batch: 24 / 125 loss: 1.266837239265442 mean_loss: 1.3849459695816038 time_taken: 7.707745313644409\n","batch: 25 / 125 loss: 1.275356411933899 mean_loss: 1.3807309865951536 time_taken: 7.6530210971832275\n","batch: 26 / 125 loss: 1.2610670328140259 mean_loss: 1.3762989883069636 time_taken: 7.587025880813599\n","batch: 27 / 125 loss: 1.2508724927902222 mean_loss: 1.3718194706099371 time_taken: 7.670925617218018\n","batch: 28 / 125 loss: 1.249116063117981 mean_loss: 1.367588318627456 time_taken: 7.5123114585876465\n","batch: 29 / 125 loss: 1.245381236076355 mean_loss: 1.363514749209086 time_taken: 7.84825873374939\n","batch: 30 / 125 loss: 1.235764741897583 mean_loss: 1.3593937812312955 time_taken: 7.877076148986816\n","batch: 31 / 125 loss: 1.2263479232788086 mean_loss: 1.3552360981702802 time_taken: 7.71885085105896\n","batch: 32 / 125 loss: 1.2197034358978271 mean_loss: 1.3511290477983877 time_taken: 7.691303253173828\n","batch: 33 / 125 loss: 1.2099453210830688 mean_loss: 1.3469765852479372 time_taken: 7.494691610336304\n","batch: 34 / 125 loss: 1.2145522832870483 mean_loss: 1.3431930337633404 time_taken: 7.541669607162476\n","batch: 35 / 125 loss: 1.2001811265945435 mean_loss: 1.3392204807864294 time_taken: 7.710723638534546\n","batch: 36 / 125 loss: 1.1986209154129028 mean_loss: 1.3354204925330908 time_taken: 7.697327613830566\n","batch: 37 / 125 loss: 1.1946420669555664 mean_loss: 1.331715797123156 time_taken: 7.778122186660767\n","batch: 38 / 125 loss: 1.1889629364013672 mean_loss: 1.3280554673610587 time_taken: 7.491974353790283\n","batch: 39 / 125 loss: 1.1823848485946655 mean_loss: 1.3244137018918989 time_taken: 7.544654369354248\n","batch: 40 / 125 loss: 1.1720941066741943 mean_loss: 1.3206985898134183 time_taken: 7.632411003112793\n","batch: 41 / 125 loss: 1.174822449684143 mean_loss: 1.3172253483817689 time_taken: 7.771677732467651\n","batch: 42 / 125 loss: 1.171317219734192 mean_loss: 1.3138321360876393 time_taken: 7.719953298568726\n","batch: 43 / 125 loss: 1.1496496200561523 mean_loss: 1.3101007152687418 time_taken: 7.74480938911438\n","batch: 44 / 125 loss: 1.1607991456985474 mean_loss: 1.3067829026116264 time_taken: 7.759071350097656\n","batch: 45 / 125 loss: 1.1533905267715454 mean_loss: 1.3034482857455376 time_taken: 7.413090944290161\n","batch: 46 / 125 loss: 1.1455999612808228 mean_loss: 1.3000898107569268 time_taken: 7.448423862457275\n","batch: 47 / 125 loss: 1.1395447254180908 mean_loss: 1.2967451214790344 time_taken: 7.542114496231079\n","batch: 48 / 125 loss: 1.1335805654525757 mean_loss: 1.2934152325805353 time_taken: 7.542821407318115\n","batch: 49 / 125 loss: 1.1318950653076172 mean_loss: 1.2901848292350768 time_taken: 7.515181541442871\n","batch: 50 / 125 loss: 1.1352143287658691 mean_loss: 1.2871461919709748 time_taken: 7.483510494232178\n","batch: 51 / 125 loss: 1.1116251945495605 mean_loss: 1.2837707881744091 time_taken: 7.552745580673218\n","batch: 52 / 125 loss: 1.1065595149993896 mean_loss: 1.2804271792465787 time_taken: 7.856135129928589\n","batch: 53 / 125 loss: 1.1324788331985474 mean_loss: 1.277687395060504 time_taken: 7.792698860168457\n","batch: 54 / 125 loss: 1.1024671792984009 mean_loss: 1.2745015729557385 time_taken: 7.535808324813843\n","batch: 55 / 125 loss: 1.0976240634918213 mean_loss: 1.2713430460010258 time_taken: 7.6618812084198\n","batch: 56 / 125 loss: 1.1272507905960083 mean_loss: 1.2688151116956745 time_taken: 8.095926284790039\n","batch: 57 / 125 loss: 1.0975958108901978 mean_loss: 1.2658630547852352 time_taken: 8.147879362106323\n","batch: 58 / 125 loss: 1.0957982540130615 mean_loss: 1.2629806005348594 time_taken: 7.749497175216675\n","batch: 59 / 125 loss: 1.0840580463409424 mean_loss: 1.2599985579649609 time_taken: 7.692829132080078\n","batch: 60 / 125 loss: 1.0855311155319214 mean_loss: 1.257138435957862 time_taken: 7.73502254486084\n","batch: 61 / 125 loss: 1.094459891319275 mean_loss: 1.2545145884636912 time_taken: 8.234510660171509\n","batch: 62 / 125 loss: 1.079457402229309 mean_loss: 1.2517359029679074 time_taken: 7.644586563110352\n","batch: 63 / 125 loss: 1.0783042907714844 mean_loss: 1.2490260340273382 time_taken: 7.592817068099976\n","batch: 64 / 125 loss: 1.0763638019561768 mean_loss: 1.2463696919954743 time_taken: 7.5419135093688965\n","batch: 65 / 125 loss: 1.0677566528320312 mean_loss: 1.2436634338263313 time_taken: 7.527012825012207\n","batch: 66 / 125 loss: 1.070773959159851 mean_loss: 1.241082993905936 time_taken: 7.768454074859619\n","batch: 67 / 125 loss: 1.0646653175354004 mean_loss: 1.2384886163122517 time_taken: 7.635111331939697\n","batch: 68 / 125 loss: 1.0776938199996948 mean_loss: 1.236158256945403 time_taken: 7.431746006011963\n","batch: 69 / 125 loss: 1.0615543127059937 mean_loss: 1.23366391488484 time_taken: 7.6686179637908936\n","batch: 70 / 125 loss: 1.0653129816055298 mean_loss: 1.2312927749794977 time_taken: 7.760924577713013\n","batch: 71 / 125 loss: 1.0601143836975098 mean_loss: 1.2289152973228035 time_taken: 7.649750709533691\n","batch: 72 / 125 loss: 1.04548978805542 mean_loss: 1.2264026191136612 time_taken: 7.5480170249938965\n","batch: 73 / 125 loss: 1.052189826965332 mean_loss: 1.2240483921927379 time_taken: 7.565030574798584\n","batch: 74 / 125 loss: 1.038462519645691 mean_loss: 1.2215739138921107 time_taken: 7.7328712940216064\n","batch: 75 / 125 loss: 1.0398194789886475 mean_loss: 1.2191824081696967 time_taken: 7.690223693847656\n","batch: 76 / 125 loss: 1.037295937538147 mean_loss: 1.2168202462134428 time_taken: 7.463099956512451\n","batch: 77 / 125 loss: 1.0341715812683105 mean_loss: 1.2144785966628642 time_taken: 7.510403394699097\n","batch: 78 / 125 loss: 1.0315473079681396 mean_loss: 1.2121630107300196 time_taken: 7.757512092590332\n","batch: 79 / 125 loss: 1.0290601253509521 mean_loss: 1.2098742246627814 time_taken: 7.510704517364502\n","batch: 80 / 125 loss: 1.020363211631775 mean_loss: 1.2075345825265962 time_taken: 7.40629506111145\n","batch: 81 / 125 loss: 1.0299285650253296 mean_loss: 1.2053686554838978 time_taken: 7.3925275802612305\n","batch: 82 / 125 loss: 1.0191136598587036 mean_loss: 1.203124619392028 time_taken: 7.564777851104736\n","batch: 83 / 125 loss: 1.01748788356781 mean_loss: 1.2009146582512635 time_taken: 7.324295520782471\n","batch: 84 / 125 loss: 1.0137635469436646 mean_loss: 1.1987128804711742 time_taken: 7.798899173736572\n","batch: 85 / 125 loss: 1.0104914903640747 mean_loss: 1.1965242596559753 time_taken: 7.614749908447266\n","batch: 86 / 125 loss: 1.0113177299499512 mean_loss: 1.194395448969699 time_taken: 7.57328987121582\n","batch: 87 / 125 loss: 1.0130469799041748 mean_loss: 1.1923346709121363 time_taken: 7.530261516571045\n","batch: 88 / 125 loss: 1.0150104761123657 mean_loss: 1.1903422642289927 time_taken: 7.51949405670166\n","batch: 89 / 125 loss: 1.0067589282989502 mean_loss: 1.1883024493853256 time_taken: 7.70529842376709\n","batch: 90 / 125 loss: 1.0065263509750366 mean_loss: 1.1863049098423555 time_taken: 7.741596221923828\n","batch: 91 / 125 loss: 0.9951883554458618 mean_loss: 1.1842275559902198 time_taken: 7.585346698760986\n","batch: 92 / 125 loss: 0.9989476799964905 mean_loss: 1.1822352992591043 time_taken: 7.8241565227508545\n","batch: 93 / 125 loss: 1.0033814907073975 mean_loss: 1.1803325991681286 time_taken: 7.673722982406616\n","batch: 94 / 125 loss: 0.9959037899971008 mean_loss: 1.1783912432821178 time_taken: 7.648332357406616\n","batch: 95 / 125 loss: 1.009552240371704 mean_loss: 1.1766325036684675 time_taken: 7.598892450332642\n","batch: 96 / 125 loss: 0.9908289909362793 mean_loss: 1.1747170035372079 time_taken: 7.705887317657471\n","batch: 97 / 125 loss: 0.9907053709030151 mean_loss: 1.172839333816451 time_taken: 7.829730272293091\n","batch: 98 / 125 loss: 0.98647141456604 mean_loss: 1.1709568295815982 time_taken: 7.697495698928833\n","batch: 99 / 125 loss: 0.9855395555496216 mean_loss: 1.1691026568412783 time_taken: 7.617074012756348\n","batch: 100 / 125 loss: 0.9904555082321167 mean_loss: 1.1673338731916827 time_taken: 7.376347064971924\n","batch: 101 / 125 loss: 0.9856010675430298 mean_loss: 1.1655521790186567 time_taken: 7.246246337890625\n","batch: 102 / 125 loss: 0.9936938285827637 mean_loss: 1.1638836513445219 time_taken: 7.332936525344849\n","batch: 103 / 125 loss: 0.9788077473640442 mean_loss: 1.1621040753447096 time_taken: 7.597118139266968\n","batch: 104 / 125 loss: 0.985373318195343 mean_loss: 1.1604209252766204 time_taken: 7.583251237869263\n","batch: 105 / 125 loss: 0.9818689227104187 mean_loss: 1.1587364724222222 time_taken: 7.656218528747559\n","batch: 106 / 125 loss: 0.9816536903381348 mean_loss: 1.1570814931504083 time_taken: 7.499603748321533\n","batch: 107 / 125 loss: 0.9793130159378052 mean_loss: 1.155435488731773 time_taken: 7.60931658744812\n","batch: 108 / 125 loss: 0.9812424778938293 mean_loss: 1.153837387714911 time_taken: 7.518133163452148\n","batch: 109 / 125 loss: 0.9702845811843872 mean_loss: 1.1521687258373607 time_taken: 7.795621871948242\n","batch: 110 / 125 loss: 0.971110999584198 mean_loss: 1.1505375751503952 time_taken: 7.609435558319092\n","batch: 111 / 125 loss: 0.9772435426712036 mean_loss: 1.1489903070032597 time_taken: 7.305767774581909\n","batch: 112 / 125 loss: 0.9759042263031006 mean_loss: 1.1474585717758246 time_taken: 7.4893271923065186\n","batch: 113 / 125 loss: 0.9794373512268066 mean_loss: 1.1459847014201314 time_taken: 7.4254114627838135\n","batch: 114 / 125 loss: 0.9799411296844482 mean_loss: 1.1445408442746037 time_taken: 7.642895221710205\n","batch: 115 / 125 loss: 0.9647389054298401 mean_loss: 1.1429908275604246 time_taken: 7.569788217544556\n","batch: 116 / 125 loss: 0.9640865325927734 mean_loss: 1.1414617310222395 time_taken: 7.743697881698608\n","batch: 117 / 125 loss: 0.9703221917152405 mean_loss: 1.1400113959433666 time_taken: 7.424323320388794\n","batch: 118 / 125 loss: 0.9732231497764587 mean_loss: 1.1386098140428045 time_taken: 7.560215711593628\n","batch: 119 / 125 loss: 0.9688116312026978 mean_loss: 1.1371948291858036 time_taken: 7.405874729156494\n","batch: 120 / 125 loss: 0.9705637097358704 mean_loss: 1.1358177124961348 time_taken: 7.709294080734253\n","batch: 121 / 125 loss: 0.9653183221817017 mean_loss: 1.1344201765099509 time_taken: 7.471357107162476\n","batch: 122 / 125 loss: 0.9678425788879395 mean_loss: 1.1330658870983898 time_taken: 7.509666442871094\n","batch: 123 / 125 loss: 0.964860737323761 mean_loss: 1.131709393955046 time_taken: 7.285062789916992\n","batch: 124 / 125 loss: 0.962061882019043 mean_loss: 1.1303522138595579 time_taken: 7.575032472610474\n","<built-in method size of Tensor object at 0x7f9cd871e558>\n","<built-in method size of Tensor object at 0x7f9cd871e558>\n","<built-in method size of Tensor object at 0x7f9cd871eb88>\n","<built-in method size of Tensor object at 0x7f9cd871eea0>\n","<built-in method size of Tensor object at 0x7f9cd872aea0>\n","epch 1 / 2: Test Pixel Accuracy = 0.949 max_accuracy = 0.949 in epoch 1\n","batch: 0 / 125 loss: 0.9777556657791138 mean_loss: 1.1291411301446337 time_taken: 7.992417097091675\n","batch: 1 / 125 loss: 0.9645020961761475 mean_loss: 1.1278447597984251 time_taken: 7.3917601108551025\n","batch: 2 / 125 loss: 0.9638985991477966 mean_loss: 1.1265639304183421 time_taken: 7.453660011291504\n","batch: 3 / 125 loss: 0.9656481742858887 mean_loss: 1.125316521456075 time_taken: 7.6342737674713135\n","batch: 4 / 125 loss: 0.9626725912094116 mean_loss: 1.1240654143003315 time_taken: 7.69167423248291\n","batch: 5 / 125 loss: 0.963829755783081 mean_loss: 1.122842241334551 time_taken: 7.4753077030181885\n","batch: 6 / 125 loss: 0.9577947854995728 mean_loss: 1.1215918818206496 time_taken: 7.534502029418945\n","batch: 7 / 125 loss: 0.9609158635139465 mean_loss: 1.1203837914574413 time_taken: 7.663626194000244\n","batch: 8 / 125 loss: 0.9608159065246582 mean_loss: 1.1191929863460026 time_taken: 7.46671199798584\n","batch: 9 / 125 loss: 0.9572055339813232 mean_loss: 1.1179930792914494 time_taken: 7.1418678760528564\n","batch: 10 / 125 loss: 0.9544570446014404 mean_loss: 1.1167906084481405 time_taken: 7.4044153690338135\n","batch: 11 / 125 loss: 0.9535959959030151 mean_loss: 1.115599406896716 time_taken: 7.497707843780518\n","batch: 12 / 125 loss: 0.9462583661079407 mean_loss: 1.114372297905493 time_taken: 7.698644638061523\n","batch: 13 / 125 loss: 0.9456456303596497 mean_loss: 1.113158436987897 time_taken: 7.598041534423828\n","batch: 14 / 125 loss: 0.9524163603782654 mean_loss: 1.1120102792978284 time_taken: 7.5237367153167725\n","batch: 15 / 125 loss: 0.9473380446434021 mean_loss: 1.11084239110879 time_taken: 7.46557092666626\n","batch: 16 / 125 loss: 0.9515106081962585 mean_loss: 1.1097203362995467 time_taken: 7.564508676528931\n","batch: 17 / 125 loss: 0.944596529006958 mean_loss: 1.1085656243604376 time_taken: 7.3561012744903564\n","batch: 18 / 125 loss: 0.9515714049339294 mean_loss: 1.1074753867255314 time_taken: 7.6431779861450195\n","batch: 19 / 125 loss: 0.942258358001709 mean_loss: 1.1063359589412292 time_taken: 7.474389314651489\n","batch: 20 / 125 loss: 0.9460480809211731 mean_loss: 1.1052380967630095 time_taken: 7.526338577270508\n","batch: 21 / 125 loss: 0.9443029761314392 mean_loss: 1.1041433000240193 time_taken: 7.4251549243927\n","batch: 22 / 125 loss: 0.9435788989067078 mean_loss: 1.1030584054218753 time_taken: 7.533735990524292\n","batch: 23 / 125 loss: 0.9410804510116577 mean_loss: 1.101971305056706 time_taken: 7.395981073379517\n","batch: 24 / 125 loss: 0.9404141306877136 mean_loss: 1.1008942572275793 time_taken: 7.316194772720337\n","batch: 25 / 125 loss: 0.9520460367202759 mean_loss: 1.0999085074228951 time_taken: 7.36849308013916\n","batch: 26 / 125 loss: 0.9406601786613464 mean_loss: 1.098860821049464 time_taken: 7.347710371017456\n","batch: 27 / 125 loss: 0.9418362379074097 mean_loss: 1.0978345165844832 time_taken: 7.748276948928833\n","batch: 28 / 125 loss: 0.9402304887771606 mean_loss: 1.0968111138065135 time_taken: 7.682420969009399\n","batch: 29 / 125 loss: 0.9386065006256104 mean_loss: 1.0957904388827657 time_taken: 7.551441192626953\n","batch: 30 / 125 loss: 0.9368155598640442 mean_loss: 1.0947713691454661 time_taken: 7.542745113372803\n","batch: 31 / 125 loss: 0.9369134902954102 mean_loss: 1.093765904948969 time_taken: 7.340944766998291\n","batch: 32 / 125 loss: 0.9374579191207886 mean_loss: 1.0927766138994235 time_taken: 7.185241937637329\n","batch: 33 / 125 loss: 0.9387097358703613 mean_loss: 1.091807639823769 time_taken: 7.416902780532837\n","batch: 34 / 125 loss: 0.9402948617935181 mean_loss: 1.09086068496108 time_taken: 7.635495662689209\n","batch: 35 / 125 loss: 0.9375191330909729 mean_loss: 1.0899082529618869 time_taken: 7.553980112075806\n","batch: 36 / 125 loss: 0.9368434548377991 mean_loss: 1.088963408529022 time_taken: 7.573388338088989\n","batch: 37 / 125 loss: 0.9372225999832153 mean_loss: 1.0880324833232196 time_taken: 7.354780435562134\n","batch: 38 / 125 loss: 0.9364374876022339 mean_loss: 1.0871081235932136 time_taken: 7.490668296813965\n","batch: 39 / 125 loss: 0.93622225522995 mean_loss: 1.0861936637849514 time_taken: 7.221753835678101\n","batch: 40 / 125 loss: 0.9372056126594543 mean_loss: 1.0852961454046772 time_taken: 7.238297939300537\n","batch: 41 / 125 loss: 0.9355322122573853 mean_loss: 1.084399355385831 time_taken: 7.504445791244507\n","batch: 42 / 125 loss: 0.9346895813941956 mean_loss: 1.083508225778738 time_taken: 7.656057119369507\n","batch: 43 / 125 loss: 0.9340988993644714 mean_loss: 1.0826241469242157 time_taken: 7.44554591178894\n","batch: 44 / 125 loss: 0.9332581758499146 mean_loss: 1.0817455235649551 time_taken: 7.683911323547363\n","batch: 45 / 125 loss: 0.9338386058807373 mean_loss: 1.0808805708299598 time_taken: 7.292621612548828\n","batch: 46 / 125 loss: 0.9350696802139282 mean_loss: 1.08003283309382 time_taken: 7.61710524559021\n","batch: 47 / 125 loss: 0.9337093830108643 mean_loss: 1.0791870328043232 time_taken: 7.515530586242676\n","batch: 48 / 125 loss: 0.9364542961120605 mean_loss: 1.078366729719885 time_taken: 7.436202764511108\n","batch: 49 / 125 loss: 0.9350183010101318 mean_loss: 1.0775475958415435 time_taken: 7.584917783737183\n","batch: 50 / 125 loss: 0.9338956475257874 mean_loss: 1.0767313915897494 time_taken: 7.280262231826782\n","batch: 51 / 125 loss: 0.9342682361602783 mean_loss: 1.075926515005402 time_taken: 7.203139305114746\n","batch: 52 / 125 loss: 0.9365853667259216 mean_loss: 1.0751436995656296 time_taken: 7.169306755065918\n","batch: 53 / 125 loss: 0.9335810542106628 mean_loss: 1.0743528467982835 time_taken: 7.478542327880859\n","batch: 54 / 125 loss: 0.934139609336853 mean_loss: 1.073573884367942 time_taken: 7.190696477890015\n","batch: 55 / 125 loss: 0.9337314367294312 mean_loss: 1.0728012741599946 time_taken: 7.48871636390686\n","batch: 56 / 125 loss: 0.9316328763961792 mean_loss: 1.0720256236228307 time_taken: 7.556878089904785\n","batch: 57 / 125 loss: 0.9325153231620789 mean_loss: 1.0712632722541928 time_taken: 7.395706415176392\n","batch: 58 / 125 loss: 0.9314072728157043 mean_loss: 1.0705031853007227 time_taken: 7.329268932342529\n","batch: 59 / 125 loss: 0.9328815340995789 mean_loss: 1.0697592844834192 time_taken: 7.463317632675171\n","batch: 60 / 125 loss: 0.9329614043235779 mean_loss: 1.0690238120094415 time_taken: 7.3303186893463135\n","batch: 61 / 125 loss: 0.931036651134491 mean_loss: 1.0682859127534257 time_taken: 7.423333168029785\n","batch: 62 / 125 loss: 0.9321552515029907 mean_loss: 1.0675618134914553 time_taken: 7.668938398361206\n","batch: 63 / 125 loss: 0.9302592873573303 mean_loss: 1.0668353450992112 time_taken: 7.62026309967041\n","batch: 64 / 125 loss: 0.9316402077674866 mean_loss: 1.0661237917448336 time_taken: 7.591233968734741\n","batch: 65 / 125 loss: 0.9316039085388184 mean_loss: 1.0654194991626031 time_taken: 7.4110167026519775\n","batch: 66 / 125 loss: 0.9330512881278992 mean_loss: 1.0647300813967975 time_taken: 7.344899415969849\n","batch: 67 / 125 loss: 0.929527223110199 mean_loss: 1.0640295484523072 time_taken: 7.4669013023376465\n","batch: 68 / 125 loss: 0.9330878853797913 mean_loss: 1.0633545914261602 time_taken: 7.4860193729400635\n","batch: 69 / 125 loss: 0.9346039891242981 mean_loss: 1.0626943319271762 time_taken: 7.628194808959961\n","batch: 70 / 125 loss: 0.9306188225746155 mean_loss: 1.0620204772876223 time_taken: 7.547073602676392\n","batch: 71 / 125 loss: 0.9313885569572449 mean_loss: 1.0613573710930517 time_taken: 7.540570974349976\n","batch: 72 / 125 loss: 0.9324662685394287 mean_loss: 1.0607064059286395 time_taken: 7.6716227531433105\n","batch: 73 / 125 loss: 0.9331917762756348 mean_loss: 1.0600656288952073 time_taken: 7.260776519775391\n","batch: 74 / 125 loss: 0.9287655353546143 mean_loss: 1.0594091284275042 time_taken: 7.221139907836914\n","batch: 75 / 125 loss: 0.9292176365852356 mean_loss: 1.0587614095626172 time_taken: 7.57192063331604\n","batch: 76 / 125 loss: 0.9314847588539124 mean_loss: 1.058131327133366 time_taken: 7.342910289764404\n","batch: 77 / 125 loss: 0.9289438128471375 mean_loss: 1.0574949354373748 time_taken: 7.469566106796265\n","batch: 78 / 125 loss: 0.9302253127098083 mean_loss: 1.05687106473773 time_taken: 7.4575865268707275\n","batch: 79 / 125 loss: 0.9310507774353027 mean_loss: 1.0562573072386938 time_taken: 7.500198841094971\n","batch: 80 / 125 loss: 0.930655300617218 mean_loss: 1.0556475887599488 time_taken: 7.558017730712891\n","batch: 81 / 125 loss: 0.9288277626037598 mean_loss: 1.055034932594943 time_taken: 7.4382383823394775\n","batch: 82 / 125 loss: 0.9333945512771606 mean_loss: 1.0544501230693768 time_taken: 7.727524995803833\n","batch: 83 / 125 loss: 0.9311473369598389 mean_loss: 1.053860157585599 time_taken: 7.877587556838989\n","batch: 84 / 125 loss: 0.9288124442100525 mean_loss: 1.0532646922838107 time_taken: 7.4567344188690186\n","batch: 85 / 125 loss: 0.9298542737960815 mean_loss: 1.052679808783869 time_taken: 7.543596982955933\n","batch: 86 / 125 loss: 0.9300184845924377 mean_loss: 1.0521012176320226 time_taken: 7.445400953292847\n","batch: 87 / 125 loss: 0.9281408786773682 mean_loss: 1.0515192442097003 time_taken: 7.628560781478882\n","batch: 88 / 125 loss: 0.9272649884223938 mean_loss: 1.0509386168462083 time_taken: 8.836605310440063\n","batch: 89 / 125 loss: 0.9297016263008118 mean_loss: 1.0503747238669274 time_taken: 7.588044166564941\n","batch: 90 / 125 loss: 0.9272918701171875 mean_loss: 1.0498048958403083 time_taken: 7.262722730636597\n","batch: 91 / 125 loss: 0.9281944036483765 mean_loss: 1.049244478825599 time_taken: 7.372860670089722\n","batch: 92 / 125 loss: 0.9284365773200989 mean_loss: 1.048690314139794 time_taken: 7.347177028656006\n","batch: 93 / 125 loss: 0.9281123280525208 mean_loss: 1.0481397297284367 time_taken: 7.431400775909424\n","batch: 94 / 125 loss: 0.9281395673751831 mean_loss: 1.0475942744450129 time_taken: 7.496924877166748\n","batch: 95 / 125 loss: 0.9278245568275452 mean_loss: 1.0470523300214043 time_taken: 7.523501396179199\n","batch: 96 / 125 loss: 0.9285281896591187 mean_loss: 1.0465184374972498 time_taken: 7.339348077774048\n","batch: 97 / 125 loss: 0.9278849959373474 mean_loss: 1.0459864489700754 time_taken: 7.124068737030029\n","batch: 98 / 125 loss: 0.9266709685325623 mean_loss: 1.045453790575265 time_taken: 7.419450998306274\n","batch: 99 / 125 loss: 0.9264310002326965 mean_loss: 1.0449248003959646 time_taken: 7.432099103927612\n","batch: 100 / 125 loss: 0.9265080690383911 mean_loss: 1.0444008325580991 time_taken: 7.72772216796875\n","batch: 101 / 125 loss: 0.9202777743339539 mean_loss: 1.043854034944777 time_taken: 7.488165378570557\n","batch: 102 / 125 loss: 0.9261613488197327 mean_loss: 1.0433378389530004 time_taken: 7.638700008392334\n","batch: 103 / 125 loss: 0.9249065518379211 mean_loss: 1.042820671760358 time_taken: 7.328531742095947\n","batch: 104 / 125 loss: 0.9267808198928833 mean_loss: 1.042316150665282 time_taken: 7.282275676727295\n","batch: 105 / 125 loss: 0.9266316294670105 mean_loss: 1.041815351872216 time_taken: 7.5360729694366455\n","batch: 106 / 125 loss: 0.9250454902648926 mean_loss: 1.0413120335032189 time_taken: 7.510252237319946\n","batch: 107 / 125 loss: 0.9248110055923462 mean_loss: 1.0408120290915843 time_taken: 7.305048227310181\n","batch: 108 / 125 loss: 0.9275761842727661 mean_loss: 1.0403281152248371 time_taken: 7.375855922698975\n","batch: 109 / 125 loss: 0.9245125651359558 mean_loss: 1.0398352830967994 time_taken: 7.592170715332031\n","batch: 110 / 125 loss: 0.9259580969810486 mean_loss: 1.0393527526471564 time_taken: 7.484287977218628\n","batch: 111 / 125 loss: 0.9189788699150085 mean_loss: 1.0388448459689616 time_taken: 7.462974786758423\n","batch: 112 / 125 loss: 0.9178888201713562 mean_loss: 1.0383366273731733 time_taken: 7.483166694641113\n","batch: 113 / 125 loss: 0.924792468547821 mean_loss: 1.0378615472107242 time_taken: 7.519120931625366\n","batch: 114 / 125 loss: 0.9279173016548157 mean_loss: 1.0374034461875745 time_taken: 7.517434597015381\n","batch: 115 / 125 loss: 0.9268115758895874 mean_loss: 1.036944558758952 time_taken: 7.4808855056762695\n","batch: 116 / 125 loss: 0.9178946018218994 mean_loss: 1.0364526167881378 time_taken: 7.769813537597656\n","batch: 117 / 125 loss: 0.9285248517990112 mean_loss: 1.0360084696071126 time_taken: 7.4335527420043945\n","batch: 118 / 125 loss: 0.9224448204040527 mean_loss: 1.0355430448152967 time_taken: 7.425479173660278\n","batch: 119 / 125 loss: 0.9230619668960571 mean_loss: 1.03508393837481 time_taken: 7.533932209014893\n","batch: 120 / 125 loss: 0.9268890619277954 mean_loss: 1.0346441218038873 time_taken: 7.563928127288818\n","batch: 121 / 125 loss: 0.928757905960083 mean_loss: 1.0342154326709165 time_taken: 7.458057165145874\n","batch: 122 / 125 loss: 0.9234134554862976 mean_loss: 1.0337686505048496 time_taken: 7.769049406051636\n","batch: 123 / 125 loss: 0.9350736141204834 mean_loss: 1.0333722848968803 time_taken: 7.56525182723999\n","batch: 124 / 125 loss: 0.9250372648239136 mean_loss: 1.0329389448165884 time_taken: 7.506732225418091\n","<built-in method size of Tensor object at 0x7f9cd8727090>\n","<built-in method size of Tensor object at 0x7f9cd871ee58>\n","<built-in method size of Tensor object at 0x7f9cd871eb40>\n","<built-in method size of Tensor object at 0x7f9cd871eaf8>\n","<built-in method size of Tensor object at 0x7f9cd871edc8>\n","epch 2 / 2: Test Pixel Accuracy = 0.953 max_accuracy = 0.953 in epoch 2\n","Done training. Weights saved to: model.pt\n","Evaluating on test set\n","<built-in method size of Tensor object at 0x7f9cd8730a68>\n","<built-in method size of Tensor object at 0x7f9cd8730750>\n","<built-in method size of Tensor object at 0x7f9cd87309d8>\n","<built-in method size of Tensor object at 0x7f9cd8730a68>\n","<built-in method size of Tensor object at 0x7f9cd8730750>\n","Test Pixel Accuracy = 0.953\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"IBaXJdvg_ZXV"},"source":["#Possible Output \n","Training on GPU: Tesla K80\n","\n","Training...\n","\n","epch 1 / 25: Test Pixel Accuracy = 0.865 max_accuracy = 0.865 in epoch 1\n","\n","epch 2 / 25: Test Pixel Accuracy = 0.945 max_accuracy = 0.945 in epoch 2\n","\n","epch 3 / 25: Test Pixel Accuracy = 0.938 max_accuracy = 0.945 in epoch 2\n","\n","epch 4 / 25: Test Pixel Accuracy = 0.975 max_accuracy = 0.975 in epoch 4\n","\n","epch 5 / 25: Test Pixel Accuracy = 0.981 max_accuracy = 0.981 in epoch 5\n","\n","epch 6 / 25: Test Pixel Accuracy = 0.982 max_accuracy = 0.982 in epoch 6\n","\n","epch 7 / 25: Test Pixel Accuracy = 0.583 max_accuracy = 0.982 in epoch 6\n","\n","epch 8 / 25: Test Pixel Accuracy = 0.959 max_accuracy = 0.982 in epoch 6\n","\n","epch 9 / 25: Test Pixel Accuracy = 0.762 max_accuracy = 0.982 in epoch 6\n","\n","epch 10 / 25: Test Pixel Accuracy = 0.864 max_accuracy = 0.982 in epoch 6\n","\n","epch 11 / 25: Test Pixel Accuracy = 0.941 max_accuracy = 0.982 in epoch 6\n","\n","epch 12 / 25: Test Pixel Accuracy = 0.963 max_accuracy = 0.982 in epoch 6\n","\n","epch 13 / 25: Test Pixel Accuracy = 0.954 max_accuracy = 0.982 in epoch 6\n","\n","epch 14 / 25: Test Pixel Accuracy = 0.821 max_accuracy = 0.982 in epoch 6\n","\n","epch 15 / 25: Test Pixel Accuracy = 0.846 max_accuracy = 0.982 in epoch 6\n","\n","epch 16 / 25: Test Pixel Accuracy = 0.967 max_accuracy = 0.982 in epoch 6\n","\n","epch 17 / 25: Test Pixel Accuracy = 0.945 max_accuracy = 0.982 in epoch 6\n","\n","epch 18 / 25: Test Pixel Accuracy = 0.971 max_accuracy = 0.982 in epoch 6\n","\n","epch 19 / 25: Test Pixel Accuracy = 0.985 max_accuracy = 0.985 in epoch 19\n","\n","epch 20 / 25: Test Pixel Accuracy = 0.980 max_accuracy = 0.985 in epoch 19\n","\n","epch 21 / 25: Test Pixel Accuracy = 0.986 max_accuracy = 0.986 in epoch 21\n","\n","epch 22 / 25: Test Pixel Accuracy = 0.988 max_accuracy = 0.988 in epoch 22\n","\n","epch 23 / 25: Test Pixel Accuracy = 0.989 max_accuracy = 0.989 in epoch 23\n","\n","epch 24 / 25: Test Pixel Accuracy = 0.982 max_accuracy = 0.989 in epoch 23\n","\n","epch 25 / 25: Test Pixel Accuracy = 0.987 max_accuracy = 0.989 in epoch 23\n","\n","Done training. Weights saved to: model.pt\n","\n","Evaluating on test set\n","\n","Test Pixel Accuracy = 0.987"]}]}